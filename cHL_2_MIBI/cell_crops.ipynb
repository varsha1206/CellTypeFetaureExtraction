{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchvision\n",
      "  Downloading torchvision-0.21.0-cp311-cp311-win_amd64.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\varsha\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torchvision) (1.26.4)\n",
      "Collecting torch==2.6.0 (from torchvision)\n",
      "  Downloading torch-2.6.0-cp311-cp311-win_amd64.whl.metadata (28 kB)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\varsha\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torchvision) (10.4.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\varsha\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch==2.6.0->torchvision) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\varsha\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch==2.6.0->torchvision) (4.12.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\varsha\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch==2.6.0->torchvision) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\varsha\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch==2.6.0->torchvision) (3.1.5)\n",
      "Requirement already satisfied: fsspec in c:\\users\\varsha\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch==2.6.0->torchvision) (2024.12.0)\n",
      "Collecting sympy==1.13.1 (from torch==2.6.0->torchvision)\n",
      "  Downloading sympy-1.13.1-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\varsha\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sympy==1.13.1->torch==2.6.0->torchvision) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\varsha\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jinja2->torch==2.6.0->torchvision) (3.0.2)\n",
      "Downloading torchvision-0.21.0-cp311-cp311-win_amd64.whl (1.6 MB)\n",
      "   ---------------------------------------- 0.0/1.6 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.6/1.6 MB 16.6 MB/s eta 0:00:00\n",
      "Downloading torch-2.6.0-cp311-cp311-win_amd64.whl (204.2 MB)\n",
      "   ---------------------------------------- 0.0/204.2 MB ? eta -:--:--\n",
      "   - -------------------------------------- 6.3/204.2 MB 29.7 MB/s eta 0:00:07\n",
      "   -- ------------------------------------- 14.2/204.2 MB 35.6 MB/s eta 0:00:06\n",
      "   ---- ----------------------------------- 22.5/204.2 MB 36.6 MB/s eta 0:00:05\n",
      "   ------ --------------------------------- 30.9/204.2 MB 37.8 MB/s eta 0:00:05\n",
      "   ------- -------------------------------- 39.6/204.2 MB 38.1 MB/s eta 0:00:05\n",
      "   --------- ------------------------------ 47.4/204.2 MB 38.2 MB/s eta 0:00:05\n",
      "   ---------- ----------------------------- 55.6/204.2 MB 38.5 MB/s eta 0:00:04\n",
      "   ------------ --------------------------- 64.0/204.2 MB 38.8 MB/s eta 0:00:04\n",
      "   -------------- ------------------------- 71.8/204.2 MB 38.8 MB/s eta 0:00:04\n",
      "   --------------- ------------------------ 80.0/204.2 MB 39.0 MB/s eta 0:00:04\n",
      "   ----------------- ---------------------- 88.6/204.2 MB 39.0 MB/s eta 0:00:03\n",
      "   ------------------- -------------------- 97.0/204.2 MB 38.9 MB/s eta 0:00:03\n",
      "   -------------------- ------------------ 104.9/204.2 MB 39.2 MB/s eta 0:00:03\n",
      "   --------------------- ----------------- 113.5/204.2 MB 39.2 MB/s eta 0:00:03\n",
      "   ----------------------- --------------- 121.6/204.2 MB 39.2 MB/s eta 0:00:03\n",
      "   ------------------------ -------------- 130.0/204.2 MB 39.2 MB/s eta 0:00:02\n",
      "   -------------------------- ------------ 137.9/204.2 MB 39.3 MB/s eta 0:00:02\n",
      "   --------------------------- ----------- 146.0/204.2 MB 39.4 MB/s eta 0:00:02\n",
      "   ----------------------------- --------- 154.4/204.2 MB 39.3 MB/s eta 0:00:02\n",
      "   ------------------------------- ------- 162.5/204.2 MB 39.3 MB/s eta 0:00:02\n",
      "   -------------------------------- ------ 171.2/204.2 MB 39.4 MB/s eta 0:00:01\n",
      "   ---------------------------------- ---- 179.6/204.2 MB 39.4 MB/s eta 0:00:01\n",
      "   ----------------------------------- --- 188.0/204.2 MB 39.4 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 196.3/204.2 MB 39.5 MB/s eta 0:00:01\n",
      "   --------------------------------------  203.9/204.2 MB 39.5 MB/s eta 0:00:01\n",
      "   --------------------------------------  203.9/204.2 MB 39.5 MB/s eta 0:00:01\n",
      "   --------------------------------------- 204.2/204.2 MB 37.2 MB/s eta 0:00:00\n",
      "Downloading sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
      "   ---------------------------------------- 0.0/6.2 MB ? eta -:--:--\n",
      "   ---------------------------------------- 6.2/6.2 MB 31.7 MB/s eta 0:00:00\n",
      "Installing collected packages: sympy, torch, torchvision\n",
      "  Attempting uninstall: sympy\n",
      "    Found existing installation: sympy 1.13.3\n",
      "    Uninstalling sympy-1.13.3:\n",
      "      Successfully uninstalled sympy-1.13.3\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 2.2.2\n",
      "    Uninstalling torch-2.2.2:\n",
      "      Successfully uninstalled torch-2.2.2\n",
      "Successfully installed sympy-1.13.1 torch-2.6.0 torchvision-0.21.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "aiagents4pharma 0.0.0 requires torch==2.2.2, but you have torch 2.6.0 which is incompatible.\n",
      "\n",
      "[notice] A new release of pip is available: 25.0 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip3 install torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Varsha\\OneDrive\\Documents\\Github\\cellType\\cHL_2_MIBI\\segmentation\\1\\H3_memSUM_noCD163_deepcell060_AutoHist_mpp1.75\n",
      "38112\n",
      "after filtering:  36208\n",
      "image stack shape :  (2048, 1536, 46)\n"
     ]
    },
    {
     "ename": "SystemError",
     "evalue": "<built-in function resize> returned a result with an exception set",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 8.80 MiB for an array with shape (224, 224, 46) and data type float32",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mSystemError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[43], line 63\u001b[0m\n\u001b[0;32m     61\u001b[0m valid_boxes \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \n\u001b[0;32m     62\u001b[0m \u001b[38;5;66;03m# Resize to 224x224 for ResNet\u001b[39;00m\n\u001b[1;32m---> 63\u001b[0m cell_crop_resized \u001b[38;5;241m=\u001b[39m \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcell_crop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m224\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m224\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;66;03m# Append the cropped image to the batch list\u001b[39;00m\n\u001b[0;32m     66\u001b[0m batch_crops\u001b[38;5;241m.\u001b[39mappend(cell_crop_resized)\n",
      "\u001b[1;31mSystemError\u001b[0m: <built-in function resize> returned a result with an exception set"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import tifffile as tiff\n",
    "import numpy as np\n",
    "from mask2bbox import BBoxes\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# Define paths\n",
    "base_path = r\"C:\\Users\\Varsha\\OneDrive\\Documents\\Github\\cellType\\cHL_2_MIBI\"  # Change to your dataset path\n",
    "output_folder = r\"C:\\Users\\Varsha\\OneDrive\\Documents\\Github\\cellType\\cHL_2_MIBI\\cell_index.csv\"  # Where to save cell crops\n",
    "image_folders = [\"1\", \"2\", \"3\", \"4\", \"5\", \"6\"]  # Image folder names\n",
    "output_dir = r\"C:\\Users\\Varsha\\OneDrive\\Documents\\Github\\cellType\\cHL_2_MIBI\\cellCrops\"\n",
    "\n",
    "all_data=[]\n",
    "\n",
    "# Ensure output directory exists\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "valid_boxes = 0\n",
    "i=1\n",
    "# Iterate over each image folder\n",
    "for folder in image_folders:\n",
    "    batch_crops = [] \n",
    "    img_dir = os.path.join(base_path, f\"raw_image\\{folder}\")  # Adjust if needed\n",
    "    seg_dir = os.path.join(base_path, f\"segmentation\\{folder}\\H3_memSUM_noCD163_deepcell060_AutoHist_mpp1.75\")\n",
    "    print(seg_dir)\n",
    "    # Get all channel images and segmentation masks\n",
    "    img_paths = sorted(Path(img_dir).glob(\"*.tiff\"))  # 46 channels per image\n",
    "    mask_path = next(Path(seg_dir).glob(\"*segmentationMap.tif\"), None)  # Single mask\n",
    "\n",
    "\n",
    "    if mask_path is None:\n",
    "        print(f\"No segmentation mask found for folder {folder}, skipping.\")\n",
    "        continue\n",
    "\n",
    "    # Load segmentation mask\n",
    "    mask = tiff.imread(str(mask_path))\n",
    "    # Get bounding boxes from segmentation mask\n",
    "    all_boxes = BBoxes.from_mask(str(mask_path))\n",
    "    print(len(all_boxes))\n",
    "    # print(dir(all_boxes))\n",
    "    # Remove the bounding boxes that are located on the edge of the image\n",
    "    all_boxes = all_boxes.remove_from_edge()\n",
    "    all_boxes = all_boxes.filter(\"sides\",np.greater_equal,(5,5))\n",
    "    print(\"after filtering: \",len(all_boxes))\n",
    "    all_boxes = all_boxes.bboxes\n",
    "\n",
    "    # Load all 46 channels as a single stacked array\n",
    "    images = [tiff.imread(str(p)) for p in img_paths]\n",
    "    image_stack = np.stack(images, axis=-1)  # Shape: (H, W, 46)\n",
    "    print(\"image stack shape : \",image_stack.shape)\n",
    "    \n",
    "    # Iterate over detected cells\n",
    "    for idx, bbox in enumerate(all_boxes):\n",
    "        index, x1, y1, x2, y2 = map(int,bbox)  # Bounding box coordinates\n",
    "        # Crop across all channels\n",
    "        cell_crop = image_stack[y1:y2, x1:x2, :]  # Shape: (h, w, 46)\n",
    "        if cell_crop.size == 0:\n",
    "            continue\n",
    "\n",
    "        valid_boxes += 1 \n",
    "        # Resize to 224x224 for ResNet\n",
    "        cell_crop_resized = cv2.resize(cell_crop, (224, 224))\n",
    "\n",
    "        # Append the cropped image to the batch list\n",
    "        batch_crops.append(cell_crop_resized)\n",
    "\n",
    "        # Store metadata in CSV\n",
    "        all_data.append([folder, i])\n",
    "        i+=1\n",
    "\n",
    "    # Save the batch of crops as a single .npy file\n",
    "    if batch_crops:\n",
    "        batch_crops = np.array(batch_crops)\n",
    "        np.save(output_folder, batch_crops)\n",
    "        print(f\"Saved {batch_crops.shape[0]} cell crops to {output_folder}\")\n",
    "\n",
    "    print(f\"Processed folder {folder}, extracted {len(all_boxes)} cells.\")\n",
    "    print(\"Valid crops obtained: \", valid_boxes)\n",
    "    break\n",
    "\n",
    "\n",
    "print(\"All npy: \",len(all_data))\n",
    "\n",
    "print(\"Cell extraction complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Varsha\\OneDrive\\Documents\\Github\\cellType\\cHL_2_MIBI\\segmentation\\1\\H3_memSUM_noCD163_deepcell060_AutoHist_mpp1.75\n",
      "here 1 0.0 0.0 1535.0 2047.0\n",
      "[[[ 0.          0.          0.         ...  0.          0.\n",
      "    0.        ]\n",
      "  [ 0.          0.          0.         ...  0.          0.\n",
      "    0.        ]\n",
      "  [ 0.          0.          0.         ...  0.          0.\n",
      "    0.        ]\n",
      "  ...\n",
      "  [ 0.          0.          0.         ...  0.          0.\n",
      "    0.        ]\n",
      "  [ 0.          0.          0.         ...  0.          0.\n",
      "    0.        ]\n",
      "  [ 0.          0.          0.         ...  0.          0.\n",
      "    0.        ]]\n",
      "\n",
      " [[ 0.          0.          0.         ...  0.          0.\n",
      "    0.        ]\n",
      "  [ 0.          0.          0.         ...  0.          0.\n",
      "    0.        ]\n",
      "  [ 0.          0.          0.         ...  0.          0.\n",
      "    0.        ]\n",
      "  ...\n",
      "  [ 0.          0.          0.         ...  0.          0.\n",
      "    0.        ]\n",
      "  [ 0.          0.          0.         ...  0.          0.\n",
      "    0.        ]\n",
      "  [ 0.          0.          0.         ...  0.          0.\n",
      "    0.        ]]\n",
      "\n",
      " [[ 0.          0.          0.         ...  0.          0.\n",
      "    0.        ]\n",
      "  [ 0.          0.          0.         ...  0.          0.\n",
      "    0.        ]\n",
      "  [ 0.          0.          0.         ...  0.          0.\n",
      "    0.        ]\n",
      "  ...\n",
      "  [ 0.          0.          0.         ...  0.          0.\n",
      "    0.        ]\n",
      "  [ 0.          0.          0.         ...  0.          0.\n",
      "    0.        ]\n",
      "  [ 0.          0.          0.         ...  0.          0.\n",
      "    0.        ]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[ 0.         20.14342    45.71861    ...  0.          0.\n",
      "    0.        ]\n",
      "  [ 0.          5.5152745   5.599452   ...  0.         14.6457405\n",
      "    0.        ]\n",
      "  [ 0.          4.9344506   0.         ...  0.          0.\n",
      "    0.        ]\n",
      "  ...\n",
      "  [ 0.          0.          0.         ...  0.          0.\n",
      "    0.        ]\n",
      "  [ 0.         10.339233    0.         ...  0.          0.\n",
      "    3.768793  ]\n",
      "  [ 0.          9.893981    0.         ...  0.          0.\n",
      "    0.        ]]\n",
      "\n",
      " [[ 0.          0.         17.115583   ...  0.          0.\n",
      "    0.        ]\n",
      "  [ 0.         26.230227   29.544584   ...  0.          0.\n",
      "    0.        ]\n",
      "  [ 0.          5.115803    4.5840497  ...  0.          0.\n",
      "    0.        ]\n",
      "  ...\n",
      "  [ 0.          2.7383084   0.         ...  0.          0.\n",
      "   10.93715   ]\n",
      "  [ 0.          9.014863    0.         ...  0.          0.\n",
      "    2.080794  ]\n",
      "  [ 0.          3.1303835   0.         ...  0.          0.\n",
      "    0.        ]]\n",
      "\n",
      " [[ 0.          0.          0.         ...  0.          0.31748682\n",
      "    0.        ]\n",
      "  [ 0.          0.          4.9294167  ...  0.          0.\n",
      "    0.        ]\n",
      "  [ 0.          7.52417     0.         ...  0.          2.0342262\n",
      "    0.        ]\n",
      "  ...\n",
      "  [ 0.          7.752454    0.         ...  0.          0.\n",
      "    0.        ]\n",
      "  [ 0.          0.          0.         ...  0.          0.\n",
      "    0.        ]\n",
      "  [ 0.          0.          0.         ...  0.          0.\n",
      "    0.        ]]]\n",
      "Processed folder 1, extracted 1 cells.\n",
      "Cell extraction complete!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import tifffile as tiff\n",
    "import numpy as np\n",
    "from mask2bbox import BBoxes\n",
    "import torch\n",
    "from torchvision.ops import masks_to_boxes\n",
    "from pathlib import Path\n",
    "\n",
    "# Define paths\n",
    "base_path = r\"C:\\Users\\Varsha\\OneDrive\\Documents\\Github\\cellType\\cHL_2_MIBI\"  # Change to your dataset path\n",
    "output_folder = r\"C:\\Users\\Varsha\\OneDrive\\Documents\\Github\\cellType\\cHL_2_MIBI\\cellCrops\"  # Where to save cell crops\n",
    "image_folders = [\"1\", \"2\", \"3\", \"4\", \"5\", \"6\"]  # Image folder names\n",
    "\n",
    "# Ensure output directory exists\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Iterate over each image folder\n",
    "for folder in image_folders:\n",
    "    img_dir = os.path.join(base_path, f\"raw_image\\{folder}\")  # Adjust if needed\n",
    "    seg_dir = os.path.join(base_path, f\"segmentation\\{folder}\\H3_memSUM_noCD163_deepcell060_AutoHist_mpp1.75\")\n",
    "    print(seg_dir)\n",
    "    # Get all channel images and segmentation masks\n",
    "    img_paths = sorted(Path(img_dir).glob(\"*.tiff\"))  # 46 channels per image\n",
    "    mask_path = next(Path(seg_dir).glob(\"*segmentationMap.tif\"), None)  # Single mask\n",
    "\n",
    "\n",
    "    if mask_path is None:\n",
    "        print(f\"No segmentation mask found for folder {folder}, skipping.\")\n",
    "        continue\n",
    "\n",
    "    # Load segmentation mask\n",
    "    mask = tiff.imread(str(mask_path))\n",
    "\n",
    "    mask_tensor = torch.tensor(mask, dtype=torch.uint8)\n",
    "    mask_tensor = (mask_tensor > 0).type(torch.uint8)\n",
    "    mask_tensor = mask_tensor.unsqueeze(0)\n",
    "    \n",
    "\n",
    "    boxes = masks_to_boxes(mask_tensor)\n",
    "    bboxes = boxes.tolist()\n",
    "\n",
    "    # Load all 46 channels as a single stacked array\n",
    "    images = [tiff.imread(str(p)) for p in img_paths]\n",
    "    image_stack = np.stack(images, axis=-1)  # Shape: (H, W, 46)\n",
    "    # Iterate over detected cells\n",
    "    for i,bbox in enumerate(bboxes):\n",
    "        x1, y1, x2, y2 = bbox # Bounding box coordinates\n",
    "        print(\"here 1\",x1,y1,x2,y2)\n",
    "        x1, y1, x2, y2 = map(int, (x1, y1, x2, y2))\n",
    "        # Crop across all channels\n",
    "        cell_crop = image_stack[y1:y2, x1:x2, :]  # Shape: (h, w, 46)\n",
    "        print(cell_crop)\n",
    "        # Save as NumPy array\n",
    "        crop_path = os.path.join(output_folder, f\"{folder}_cell_{i}.npy\")\n",
    "        np.save(crop_path, cell_crop)\n",
    "    \n",
    "\n",
    "    print(f\"Processed folder {folder}, extracted {len(bboxes)} cells.\")\n",
    "    break\n",
    "print(\"Cell extraction complete!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
